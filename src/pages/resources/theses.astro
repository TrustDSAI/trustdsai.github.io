---
import { Thesis } from '@/components/Thesis';
import MainLayout from '../../layouts/main.astro';
---

<MainLayout content={{ title: 'Student Theses | TrustDevSecAI' }}>
  <section class="container mx-auto py-8">
    <h1 class="text-4xl font-bold mb-8">Student Theses</h1>
    
    <div class="space-y-8">
      <!-- Masters Section -->
      <div>
        <h2 class="text-2xl font-semibold mb-4">Msc</h2>
        <div class="space-y-6">
          <!-- Publication Item -->
          <!-- <Thesis
            title="Benchmarking LLM Robustness Against Prompt-Based Adversarial Attacks"
            student='João Donato'
            advisors={["João R.Campos","Leonardo Mariani"]}
            abstract="Over the past few years, Large Language Models (LLMs) have revolutionized various fields by leveraging their extensive capabilities. Their ability to generate meaningful and contextually coherent text from minimal inputs, has led to significant advancements in areas such as Natural Language Processing (NLP) and Generative Artificial Intelligence (GenAI). These models have also shown potential in assisting or even automating complex tasks, including code generation. However, as LLMs become increasingly integrated into real-world applications and are made more accessible to the general public, concerns about the security and safety of the content they output have grown significantly. Recent research has shown that these models are vulnerable to adversarial techniques, which can manipulate them into generating harmful or biased outputs. These risks highlight the need for robust evaluation methods to assess the resilience of LLMs against these threats. While some efforts have been done to benchmark LLMs’ robustness against adversarial prompt-based attacks, these approaches are often highly context-specific and lack the comprehensive components required for a robust and systematic evaluation. In this dissertation, we have explored the concepts of LLMs, security, safety, and benchmarking with the goal of establishing a systematic benchmarking methodology for evaluating and comparing the robustness of LLMs built-in security measures when faced with adversarial prompt-based attacks. The proposed methodology is composed of the key components that make up a reliable benchmark, including scenarios, workloads, metrics and attack loads and is designed to ensure its representativeness, usefulness and adaptability to different contexts. To demonstrate the usefulness of the proposed methodology, we have also conducted a benchmarking campaign with a varied set of LLMs in the context of vulnerable and also malicious code generation. From this instantiation, we uncovered significant insights into the models’ safety alignments and vulnerabilities. Our findings reveal that while most safety-aligned models effectively refuse to generate malicious code, they readily produce vulnerable code when asked. Furthermore, our analysis of various attack vectors demonstrated that multi-turn conversational attacks and single-turn role-playing scenarios are significantly more effective at bypassing safety measures than template-based prompts. These results underscore the importance of a structured benchmarking methodology and reveal key weaknesses in current LLMs, providing a clear path for future research in developing more robust and secure models."
            year="2025"
            pdf=""
          /> -->
      </div>
    </div>
  </section>
</MainLayout> 
